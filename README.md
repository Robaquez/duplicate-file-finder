# duplicate-file-finder

note: this is not my code, it was entriley generated by copilot
as part of me testing it for funsies

I have an elaborate mess of unstructured backups made at random points 
in time just by copying directory structure of my projects and documents
to another drive. Far from being really safe, still better than keeping 
everything in one place on one HDD.

As such these backups tend to contain same files again and again and occupy 
plenty of space. To add insult to injury some of them have diferrent names 
in different projects, despite being identical othwerwise.

I always wanted to be able to easily check where these duplicate files 
are stored, to see if some of them can be deleted.

The idea was to write a program that will traverse the directory structure 
and list all files with the same CRC (don't ask why I used SHA1, I just did) 
and size.

That turned out to be a perfectly sized task for playing with copilot.

Prompt was:
write python program that will:
1. traverse all directory structure starting with a given file path
2. for each file calculate sha1
3. for each file store path, file size and sha1 in an appropriate data structure
4. after traversing directory structure will display paths of all duplicate 
files identifed as having the same size and sha1

It is slow, but it does exactly what I wanted.

Not that I am in any way closer to delete these duplicates.